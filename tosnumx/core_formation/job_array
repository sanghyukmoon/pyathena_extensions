#!/bin/bash
#SBATCH --job-name=ppM5J2N512        # create a short name for your job
#SBATCH --nodes=1                  # node count
#SBATCH --ntasks=1                 # total number of tasks across all nodes
#SBATCH --cpus-per-task=5          # cpu-cores per task (>1 if multi-threaded tasks)
#SBATCH --partition=bigmem
#SBATCH --mail-type=end,fail
#SBATCH --mail-user=sanghyuk.moon@princeton.edu
#SBATCH --mem-per-cpu=20G         # memory per cpu
#SBATCH --time=05:00:00          # total run time limit (HH:MM:SS)
#SBATCH --array=10-13

#######SBATCH --mem=100G               # memory per node


module purge
module load anaconda3/2022.5
conda activate pyathena

# ===== 1. Array over pid =====
MODEL=${SLURM_JOB_NAME#pp}
TASK="--radial-profile"
PID_START=1
PAR_PER_TASK=15
# ==========================
PS=$((PID_START+PAR_PER_TASK*SLURM_ARRAY_TASK_ID))
PE=$((PS+PAR_PER_TASK-1))
cmd="--unbuffered python do_tasks.py $MODEL $TASK --np $SLURM_CPUS_PER_TASK --pid-start $PS --pid-end $PE"
#echo $cmd
#srun $cmd

# ===== 2. Array over model =====
cmd="--unbuffered python do_tasks.py M5J2P${SLURM_ARRAY_TASK_ID}N512 --radial-profile --np $SLURM_CPUS_PER_TASK"
echo $cmd
srun $cmd
